{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mPR3HETPblN"
      },
      "source": [
        "## Bayesian Unobserved Components model example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "xO5j0_ShKkFI"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pymc3 as pm\n",
        "import statsmodels.api as sm\n",
        "import theano\n",
        "import theano.tensor as tt\n",
        "from pandas.plotting import register_matplotlib_converters\n",
        "from pandas_datareader.data import DataReader\n",
        "\n",
        "register_matplotlib_converters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# data = DataReader('^DJI', 'stooq', start=\"1990-01\", end=\"2024-02\")\n",
        "# data[:10]\n",
        "# print(data.shape)\n",
        "# data = data.dropna()\n",
        "# print(data.tail())\n",
        "# print(data.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Example data: US Consumer Price Index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q6PNL7ZLKmwj",
        "outputId": "29c9f029-4feb-488e-8653-3993454d275a"
      },
      "outputs": [],
      "source": [
        "cpi = DataReader(\"CPIAUCNS\", \"fred\", start=\"1971-01\", end=\"2024-02\")  #  FED: Consumer Price Index for All Urban Consumers: All Items\n",
        "cpi.index = pd.DatetimeIndex(cpi.index, freq=\"MS\")\n",
        "\n",
        "inf = np.log(cpi).resample(\"QS\").mean().diff()[1:] * 400\n",
        "inf = inf.dropna()\n",
        "\n",
        "print(inf.shape)\n",
        "print(inf.tail())\n",
        "print(inf.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        },
        "id": "RK98HTlQKu-n",
        "outputId": "8cdd9651-4e98-43c0-dba1-2129b6660ed0"
      },
      "outputs": [],
      "source": [
        "# Plot data\n",
        "fig, ax = plt.subplots(figsize=(8, 3), dpi=200)\n",
        "ax.plot(inf.index, inf, label=r\"$\\Delta \\log CPI$\", lw=1)\n",
        "ax.legend(loc=\"lower left\")\n",
        "plt.title(\"US Consumer Price Index for All Urban Consumers\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dY7ssLS1Kz_u",
        "outputId": "86d8c4d2-f10d-48c5-ea95-625eac0a89d3"
      },
      "outputs": [],
      "source": [
        "# Create an SARIMAX model instance - here we use it to estimate\n",
        "# the parameters via MLE using the `fit` method, but we can\n",
        "# also re-use it below for the Bayesian estimation\n",
        "mod = sm.tsa.statespace.SARIMAX(inf, order=(1, 0, 1))\n",
        "\n",
        "res_mle = mod.fit(disp=False)\n",
        "#print(res_mle.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 491
        },
        "id": "N2fWBJc0K5fT",
        "outputId": "39f9f3f9-9279-408b-d6e1-d091cee37bc0"
      },
      "outputs": [],
      "source": [
        "predict_mle = res_mle.get_prediction()\n",
        "predict_mle_ci = predict_mle.conf_int()\n",
        "lower = predict_mle_ci[\"lower CPIAUCNS\"]\n",
        "upper = predict_mle_ci[\"upper CPIAUCNS\"]\n",
        "\n",
        "# Graph\n",
        "fig, ax = plt.subplots(figsize=(9, 4), dpi=300)\n",
        "\n",
        "# Plot data points\n",
        "inf.plot(ax=ax, style=\"-\", label=\"Observed\")\n",
        "\n",
        "# Plot predictions\n",
        "predict_mle.predicted_mean.plot(ax=ax, style=\"r.\", label=\"One-step-ahead forecast\")\n",
        "ax.fill_between(predict_mle_ci.index, lower, upper, color=\"r\", alpha=0.1)\n",
        "ax.legend(loc=\"lower left\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Construct loglikelihood and score function for pymc:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "hhVV2xZNK-K8"
      },
      "outputs": [],
      "source": [
        "class Loglike(tt.Op):\n",
        "\n",
        "    itypes = [tt.dvector]  # expects a vector of parameter values when called\n",
        "    otypes = [tt.dscalar]  # outputs a single scalar value (the log likelihood)\n",
        "\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "        self.score = Score(self.model)\n",
        "\n",
        "    def perform(self, node, inputs, outputs):\n",
        "        (theta,) = inputs  # contains the vector of parameters\n",
        "        llf = self.model.loglike(theta)\n",
        "        outputs[0][0] = np.array(llf)  # output the log-likelihood\n",
        "\n",
        "    def grad(self, inputs, g):\n",
        "        # the method that calculates the gradients - it actually returns the\n",
        "        # vector-Jacobian product - g[0] is a vector of parameter values\n",
        "        (theta,) = inputs  # our parameters\n",
        "        out = [g[0] * self.score(theta)]\n",
        "        return out\n",
        "\n",
        "\n",
        "class Score(tt.Op):\n",
        "    itypes = [tt.dvector]\n",
        "    otypes = [tt.dvector]\n",
        "\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "\n",
        "    def perform(self, node, inputs, outputs):\n",
        "        (theta,) = inputs\n",
        "        outputs[0][0] = self.model.score(theta)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "Em6-hYLfLIkZ"
      },
      "outputs": [],
      "source": [
        "# Set sampling params\n",
        "ndraws = 3000  # number of draws from the distribution\n",
        "nburn = 600    # number of \"burn-in points\" (which will be discarded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "id": "v2_sdOzdLL-K",
        "outputId": "f4751c5c-80cb-4d35-f5dc-984743925335"
      },
      "outputs": [],
      "source": [
        "# Construct an instance of the Theano wrapper defined above, which\n",
        "# will allow PyMC3 to compute the likelihood and Jacobian in a way\n",
        "# that it can make use of. Here we are using the same model instance\n",
        "# created earlier for MLE analysis (we could also create a new model\n",
        "# instance if we preferred)\n",
        "loglike = Loglike(mod)\n",
        "\n",
        "with pm.Model() as m:\n",
        "    # Priors\n",
        "    arL1 = pm.Uniform(\"ar.L1\", -0.99, 0.99)\n",
        "    maL1 = pm.Uniform(\"ma.L1\", -0.99, 0.99)\n",
        "    sigma2 = pm.InverseGamma(\"sigma2\", 2, 4)\n",
        "\n",
        "    # convert variables to tensor vectors\n",
        "    theta = tt.as_tensor_variable([arL1, maL1, sigma2])\n",
        "\n",
        "    # use a DensityDist (use a lamdba function to \"call\" the Op)\n",
        "    pm.DensityDist(\"likelihood\", loglike, observed=theta)\n",
        "\n",
        "    # Draw samples\n",
        "    trace = pm.sample(\n",
        "        ndraws,\n",
        "        tune=nburn,\n",
        "        return_inferencedata=True,\n",
        "        cores=1,\n",
        "        compute_convergence_checks=False,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 906
        },
        "id": "TuDZ3o_hLQ1h",
        "outputId": "c754875a-a857-43ea-98d4-67782756da97"
      },
      "outputs": [],
      "source": [
        "plt.tight_layout()\n",
        "# Note: the syntax here for the lines argument is required for\n",
        "# PyMC3 versions >= 3.7\n",
        "# For version <= 3.6 you can use lines=dict(res_mle.params) instead\n",
        "_ = pm.plot_trace(\n",
        "    trace,\n",
        "    lines=[(k, {}, [v]) for k, v in dict(res_mle.params).items()],\n",
        "    combined=True,\n",
        "    figsize=(12, 12),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "yIy_OHa0Lz2Y",
        "outputId": "b4d73604-bed5-489a-95d1-40d8a3cf6f7d"
      },
      "outputs": [],
      "source": [
        "pm.summary(trace)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 491
        },
        "id": "razREv8oL0sk",
        "outputId": "ab4402e9-85d1-4d10-b147-b35eda77b03e"
      },
      "outputs": [],
      "source": [
        "# Retrieve the posterior means\n",
        "params = pm.summary(trace)[\"mean\"].values\n",
        "\n",
        "# Construct results using these posterior means as parameter values\n",
        "res_bayes = mod.smooth(params)\n",
        "\n",
        "predict_bayes = res_bayes.get_prediction()\n",
        "predict_bayes_ci = predict_bayes.conf_int()\n",
        "lower = predict_bayes_ci[\"lower CPIAUCNS\"]\n",
        "upper = predict_bayes_ci[\"upper CPIAUCNS\"]\n",
        "\n",
        "# Graph\n",
        "fig, ax = plt.subplots(figsize=(9, 4), dpi=300)\n",
        "\n",
        "# Plot data points\n",
        "inf.plot(ax=ax, style=\"-\", label=\"Observed\")\n",
        "\n",
        "# Plot predictions\n",
        "predict_bayes.predicted_mean.plot(ax=ax, style=\"r.\", label=\"One-step-ahead forecast\")\n",
        "ax.fill_between(predict_bayes_ci.index, lower, upper, color=\"r\", alpha=0.1)\n",
        "ax.legend(loc=\"lower left\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9moPqI_cO3F8",
        "outputId": "cea53b11-a65f-4435-afde-32679a1f9fd8"
      },
      "outputs": [],
      "source": [
        "# Construct the model instance\n",
        "mod_uc = sm.tsa.UnobservedComponents(inf, \"rwalk\", autoregressive=1)\n",
        "\n",
        "# Fit the model via maximum likelihood\n",
        "res_uc_mle = mod_uc.fit()\n",
        "print(res_uc_mle.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "zzlnmV66O8vi"
      },
      "outputs": [],
      "source": [
        "# Set sampling params\n",
        "ndraws = 3000  # number of draws from the distribution\n",
        "nburn = 600  # number of \"burn-in points\" (which will be discarded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "id": "-n85YArcPBoE",
        "outputId": "84403981-1119-4af8-806e-cb7dfacdfd8b"
      },
      "outputs": [],
      "source": [
        "# Here we follow the same procedure as above, but now we instantiate the\n",
        "# Theano wrapper `Loglike` with the UC model instance instead of the\n",
        "# SARIMAX model instance\n",
        "loglike_uc = Loglike(mod_uc)\n",
        "\n",
        "with pm.Model():\n",
        "    # Priors\n",
        "    sigma2level = pm.InverseGamma(\"sigma2.level\", 1, 1)\n",
        "    sigma2ar = pm.InverseGamma(\"sigma2.ar\", 1, 1)\n",
        "    arL1 = pm.Uniform(\"ar.L1\", -0.99, 0.99)\n",
        "\n",
        "    # convert variables to tensor vectors\n",
        "    theta_uc = tt.as_tensor_variable([sigma2level, sigma2ar, arL1])\n",
        "\n",
        "    # use a DensityDist (use a lamdba function to \"call\" the Op)\n",
        "    pm.DensityDist(\"likelihood\", loglike_uc, observed=theta_uc)\n",
        "\n",
        "    # Draw samples\n",
        "    trace_uc = pm.sample(\n",
        "        ndraws,\n",
        "        tune=nburn,\n",
        "        return_inferencedata=True,\n",
        "        cores=1,\n",
        "        compute_convergence_checks=False,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 906
        },
        "id": "ckhRmsD9PGfT",
        "outputId": "5aa65f43-b22f-4378-969d-e6a3d629aa8b"
      },
      "outputs": [],
      "source": [
        "plt.tight_layout()\n",
        "# Note: the syntax here for the lines argument is required for\n",
        "# PyMC3 versions >= 3.7\n",
        "# For version <= 3.6 you can use lines=dict(res_mle.params) instead\n",
        "_ = pm.plot_trace(\n",
        "    trace_uc,\n",
        "    lines=[(k, {}, [v]) for k, v in dict(res_uc_mle.params).items()],\n",
        "    combined=True,\n",
        "    figsize=(12, 12),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "z-G-pXA5PLyb",
        "outputId": "42ec7fd4-72f7-45e0-85ab-a396bcc4e60e"
      },
      "outputs": [],
      "source": [
        "pm.summary(trace_uc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "gAlgoJvHPPRB"
      },
      "outputs": [],
      "source": [
        "# Retrieve the posterior means\n",
        "params = pm.summary(trace_uc)[\"mean\"].values\n",
        "\n",
        "# Construct results using these posterior means as parameter values\n",
        "res_uc_bayes = mod_uc.smooth(params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 831
        },
        "id": "3_0m4DabPSp6",
        "outputId": "966f6c69-5ff7-44b3-8bbe-e3cddb363ee9"
      },
      "outputs": [],
      "source": [
        "predict_bayes = res_uc_bayes.get_prediction()\n",
        "predict_bayes_ci = predict_bayes.conf_int()\n",
        "\n",
        "lower = predict_bayes_ci[\"lower CPIAUCNS\"]\n",
        "upper = predict_bayes_ci[\"upper CPIAUCNS\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Graph\n",
        "fig, ax = plt.subplots(figsize=(9, 4), dpi=300)\n",
        "\n",
        "# Plot data points\n",
        "inf[\"CPIAUCNS\"].plot(ax=ax, style=\"-\", label=\"Observed data\")\n",
        "\n",
        "# Plot estimate of the level term\n",
        "res_uc_mle.states.smoothed[\"level\"].plot(ax=ax, label=\"Smoothed level (MLE)\")\n",
        "res_uc_bayes.states.smoothed[\"level\"].plot(ax=ax, label=\"Smoothed level (Bayesian)\")\n",
        "\n",
        "ax.legend(loc=\"lower left\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBuQsW7xP4J5"
      },
      "source": [
        "# AR(1):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "id": "JTNOJun3qk-s",
        "outputId": "a45b5cba-fe62-4905-e261-f0b7e4f5f3cc"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "#from mpl_toolkits.mplot3d import Axes3D\n",
        "#from scipy.stats import wishart, multivariate_normal, bernoulli, multinomial\n",
        "#from scipy.sparse import csr_matrix\n",
        "#from sklearn.model_selection import train_test_split\n",
        "import os, pickle\n",
        "import numpy as np\n",
        "import math\n",
        "#from numpy import log, sum, exp, prod\n",
        "from numpy.random import beta, binomial, normal, dirichlet, uniform, gamma, seed, multinomial, gumbel, rand\n",
        "from imp import reload\n",
        "from copy import deepcopy\n",
        "from math import cos, pi\n",
        "#import seaborn as sns\n",
        "import pandas as pd\n",
        "import time\n",
        "#from scipy.spatial.distance import euclidean\n",
        "import itertools\n",
        "from itertools import chain, combinations\n",
        "\n",
        "\n",
        "def cycle(N, nof_cycles = 1):\n",
        "  return np.cos(2*pi*np.arange(0,N)*nof_cycles/N)\n",
        "\n",
        "def simAR1(N, phi, sigma, const = 0, burn=100):\n",
        "  y = np.zeros((N+burn))\n",
        "  for t in range(N+burn-1):\n",
        "    y[t+1] = const + phi*y[t] + normal(scale = sigma, size=1)\n",
        "  return y[burn:]\n",
        "\n",
        "#np.random.seed(0)   # set seed\n",
        "\n",
        "N = 2*10**3\n",
        "omega = 1\n",
        "phi_true = 0.77\n",
        "sigma_true = 0.5\n",
        "\n",
        "y = simAR1(N, phi = phi_true, sigma = sigma_true, const = 0.5)\n",
        "#y1 = omega*cycle(N, nof_cycles = 2) + simAR1(N, phi = 0.7, sigma = 0.6)\n",
        "#y2 = omega*cycle(N, nof_cycles = 2) + simAR1(N, phi = 0.7, sigma = 0.6)\n",
        "#y3 = omega*cycle(N , nof_cycles = 2) + simAR1(N , phi = 0.5, sigma = 1.4)\n",
        "#y4 = omega*cycle(N, nof_cycles = 2)\n",
        "\n",
        "# Plot trajectories:\n",
        "#---------------------\n",
        "plt.figure()\n",
        "pd.Series(y).plot()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hIp5hVldPWm_",
        "outputId": "8703fb25-cb2a-41bb-faa2-59a910366af6"
      },
      "outputs": [],
      "source": [
        "# Construct the model instance\n",
        "mod_ar1 = sm.tsa.statespace.SARIMAX(y, trend = 'c', order=(1,0,0), seasonal_order=(0,0,0,0))\n",
        "\n",
        "# Fit the model via maximum likelihood\n",
        "res_ar_mle = mod_ar1.fit()\n",
        "print(res_ar_mle.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "-Pt2WqkPYBSR",
        "outputId": "35ad34a7-92e3-44e3-de53-1070e57a805c"
      },
      "outputs": [],
      "source": [
        "loglike = Loglike(mod_ar1)\n",
        "\n",
        "#ndraws = 3000\n",
        "\n",
        "with pm.Model() as model:\n",
        "\n",
        "    # Step 1: Define statistical model symbolically, prior and likelihood\n",
        "\n",
        "    # Priors\n",
        "    arL1 = pm.Uniform(\"ar.L1\", -0.99, 0.99)\n",
        "    intercept = pm.Uniform(\"intercept\", -2, 2)\n",
        "    sigma2 = pm.InverseGamma(\"sigma2\", 2, 4)\n",
        "\n",
        "    # set predictors as shared variable to change them for PPCs:\n",
        "    #pred = pm.Data(\"pred\", y)\n",
        "\n",
        "    # convert variables to tensor vectors\n",
        "    theta = tt.as_tensor_variable([arL1, intercept, sigma2])\n",
        "\n",
        "    # use a DensityDist (use a lamdba function to \"call\" the Op)\n",
        "    pm.DensityDist(\"likelihood\", loglike, observed=theta)\n",
        "\n",
        "    # Step 2: Inference step. Define algorithm for learning and prediction\n",
        "\n",
        "    # MCMC inference:\n",
        "    start = None #pm.find_MAP()\n",
        "    inference = pm.NUTS()\n",
        "    trace = pm.sample(ndraws, inference, start=start, tune=nburn,\n",
        "                      return_inferencedata=True, compute_convergence_checks=False)\n",
        "\n",
        "    # fit model via variational inference:\n",
        "    #inference = pm.ADVI()                   # Note, that this is a mean-field approximation so we ignore correlations in the posterior.\n",
        "    #inference = pm.SVGD(n_particles=500, jitter=1)\n",
        "\n",
        "    # tracker = pm.callbacks.Tracker(\n",
        "    #         mean = inference.approx.mean.eval,  # callable that returns mean\n",
        "    #         std = inference.approx.std.eval  # callable that returns std\n",
        "    # )\n",
        "\n",
        "    # approx = pm.fit(n=ndraws, method = inference, callbacks=[tracker])      # n: number of iterations\n",
        "    # trace = approx.sample(ndraws, inference)    # sample from the variational distr.\n",
        "\n",
        "\n",
        "    # Draw samples\n",
        "    # trace = pm.sample(\n",
        "    #     ndraws,\n",
        "    #     tune=nburn,\n",
        "    #     return_inferencedata=True,\n",
        "    #     cores=1,\n",
        "    #     compute_convergence_checks=False,\n",
        "    # )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tXuSuJm_vuqH"
      },
      "outputs": [],
      "source": [
        "def predict_outofsample(trace, date_idx):\n",
        "        \"\"\"\n",
        "        trace: a pymc3 MultiTrace object\n",
        "        date_idx: np.ndarray with shape (N_obs), indicating for each observation what date it corresponds to\n",
        "            (so you can have multiple observations on the same day that will have the same prediction)\n",
        "        \"\"\"\n",
        "        samples = []\n",
        "        horizon = np.max(date_idx)\n",
        "        for point in enumerate(trace.points()):\n",
        "            rho, scale = point['rho'], point['scale']\n",
        "            thetas = [np.random.normal(loc=0, scale=scale)]\n",
        "            for i in range(horizon):\n",
        "                thetas.append(rho*thetas[-1] + np.random.normal(loc=0, scale=scale))\n",
        "            samples.append(thetas)\n",
        "        return np.array(samples)[:, date_idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lg5tJC6rie5X"
      },
      "outputs": [],
      "source": [
        "#dir(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z7YlQ33WcIe5",
        "outputId": "8e9fb56c-1049-4c23-c4f9-bca4252c5a59"
      },
      "outputs": [],
      "source": [
        "#waic = pm.waic(trace, model)\n",
        "#print(waic)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "4lV2BVxfdojA",
        "outputId": "811f1537-0e61-4fa6-af87-2b7c1ba4154c"
      },
      "outputs": [],
      "source": [
        "# Plotting the objective function (ELBO) we can see that the optimization slowly improves the fit over time.\n",
        "plt.plot(-inference.hist, label='new ADVI', alpha=.3)\n",
        "plt.plot(approx.hist, label='old ADVI', alpha=.3)\n",
        "plt.legend()\n",
        "plt.ylabel('ELBO')\n",
        "plt.xlabel('iteration');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bqGHY8rJwq6C"
      },
      "outputs": [],
      "source": [
        "names = ['arL1', 'intercept', 'sigma2']\n",
        "dict_paras = {n:v for n,v in zip(names, res_ar_mle.params)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 961
        },
        "id": "OLRKvymeTBgF",
        "outputId": "d37f0042-d3eb-41cd-a338-4496c42e380b"
      },
      "outputs": [],
      "source": [
        "plt.tight_layout()\n",
        "\n",
        "_ = pm.plot_trace(\n",
        "    trace,\n",
        "    lines=[(k, {}, [v]) for k, v in dict_paras.items()],\n",
        "    combined=True,\n",
        "    figsize=(12, 12),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "gIZbFDZYVCD5",
        "outputId": "79d1af0a-3aaa-46e2-bd6d-e8705bc3f97b"
      },
      "outputs": [],
      "source": [
        "pm.summary(trace)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "IuhHN__-VHkq",
        "outputId": "db2af716-b951-4051-83d5-48bbaa9795f2"
      },
      "outputs": [],
      "source": [
        "trace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-RfYB6ikTUTU",
        "outputId": "a1fc49ed-590a-4651-a303-d1ddf29d5a1c"
      },
      "outputs": [],
      "source": [
        "# Retrieve the posterior means\n",
        "params = pm.summary(trace)[\"mean\"].values\n",
        "\n",
        "# Construct results using these posterior means as parameter values\n",
        "res_bayes = mod_ar1.smooth(params)\n",
        "\n",
        "predict_bayes = res_bayes.get_prediction()\n",
        "predict_bayes_ci = predict_bayes.conf_int()\n",
        "lower = predict_bayes_ci[\"lower CPIAUCNS\"]\n",
        "upper = predict_bayes_ci[\"upper CPIAUCNS\"]\n",
        "\n",
        "# Graph\n",
        "fig, ax = plt.subplots(figsize=(9, 4), dpi=300)\n",
        "\n",
        "# Plot data points\n",
        "inf.plot(ax=ax, style=\"-\", label=\"Observed\")\n",
        "\n",
        "# Plot predictions\n",
        "predict_bayes.predicted_mean.plot(ax=ax, style=\"r.\", label=\"One-step-ahead forecast\")\n",
        "ax.fill_between(predict_bayes_ci.index, lower, upper, color=\"r\", alpha=0.1)\n",
        "ax.legend(loc=\"lower left\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7w8MNWT1UmCq",
        "outputId": "ae0d7bf5-e956-4175-e137-4ba410dfaec4"
      },
      "outputs": [],
      "source": [
        "trace['posterior']['chain'][0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VsK9Z7yOU3EZ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
